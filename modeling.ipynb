{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0cb15d",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#121212; border-left: 5px solid #00bfbf; padding: 1.5em; font-family: 'Segoe UI', sans-serif; color: #e0e0e0; line-height:1.7;\">\n",
    "\n",
    "<h2 style=\"color:#00e6e6;\">üß† Plastic Surgery Abstracts: Topic Modeling & Trend Analysis</h2>\n",
    "\n",
    "<p><strong style=\"color:#fff;\">üìç Objective:</strong><br>\n",
    "To apply advanced <strong style=\"color:#ffffff;\">Natural Language Processing (NLP)</strong> techniques ‚Äî specifically <strong style=\"color:#ffffff;\">BERTopic</strong> ‚Äî to a curated dataset of plastic surgery abstracts. This project aims to <strong style=\"color:#ffffff;\">discover latent research themes</strong>, visualize their <strong style=\"color:#ffffff;\">evolution over time</strong>, and generate reproducible insights for clinical and academic interpretation.</p>\n",
    "\n",
    "<hr style=\"border:none; border-top: 1px dashed #666;\">\n",
    "\n",
    "<h3 style=\"color:#ffffff;\">üéØ Project Goals:</h3>\n",
    "<ul>\n",
    "  <li><strong>üß© Theme Discovery:</strong> Identify core topics from ~5,000 abstracts using clustering techniques.</li>\n",
    "  <li><strong>üîë Keyword Extraction:</strong> Use <code style=\"color:#111;background:#fff;padding:1px 4px;border-radius:3px;\">c-TF-IDF</code> to surface meaningful and distinct terms for each topic.</li>\n",
    "  <li><strong>üìà Trend Analysis:</strong> Track how topics emerge, decline, or dominate over the years.</li>\n",
    "  <li><strong>üß≠ Topic Relationships:</strong> Visualize topic similarity using UMAP and hierarchical dendrograms.</li>\n",
    "  <li><strong>üì¶ Deliverables:</strong> Export reproducible models (.pkl), clean code, and publication-ready visuals.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"font-size:0.95em; color:#aaa;\">\n",
    "<em>Note:</em> A rigorous preprocessing pipeline was applied including section header stripping, lemmatization, and medical-domain stopword removal to enhance topic coherence and signal clarity.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d408cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import spacy\n",
    "import re  # from regex\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets  # example module from scikit-learn\n",
    "from umap import UMAP\n",
    "import hdbscan\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "23a6eb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('E:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\data\\data\\merged_abstracts.csv',encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "86fc6469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year\n",
       "2023    907\n",
       "2022    895\n",
       "2021    735\n",
       "2019    594\n",
       "2016    366\n",
       "2018    349\n",
       "2017    326\n",
       "2014    242\n",
       "2015    238\n",
       "2020    215\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "482b02be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4867 entries, 0 to 4866\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   year        4867 non-null   int64  \n",
      " 1   abstract    4867 non-null   object \n",
      " 2   Unnamed: 2  0 non-null      float64\n",
      " 3   Unnamed: 3  0 non-null      float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 152.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8aab484c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BACKGROUND and PURPOSE: Teenagers with severe hemifacial microsomia can present with complex deformities of skin, adipose layer, mandible, and occlusion. Various individual techniques have been described to treat specific aspects of the deformity, including orthognathic surgery, distraction, bone grafting (vascularized/nonvascularized), and autologous fat transfer.1-3 Total correction of composite deficiencies, including creation of a proper skeletal foundation and soft tissue envelope, without relapse, is challenging in a single stage. We present our approach to teenagers with severe hemifacial microsomia incorporating orthognathic surgery, a rigid external distractor (RED) device, free osteoseptocutaneous fibular transfer, and fat grafting, to achieve stable skeletal and soft tissue correction.\\n      METHODS: 3 teenage patients with severe hemifacial microsomia (Pruzansky 3) were treated at Dell Children\\x19s Medical Center with a sequential multi-staged approach for both skeletal and soft tissue correction. Treatment protocol was as follows: Stage I: conventional orthognathic surgery, application of RED device and traction on the corrected mandible. Stage II: Mandible and facial soft tissue reconstruction with free fibula osteoseptocutaneous flap, removal RED device. Stage III: fibular skin paddle excision, autologous fat transfer, osseous genioplasty.\\n      RESULTS: All patients successfully underwent the multi-stage protocol. At conclusion of the protocol the patients maintained a class I occlusion without cant, a stable and symmetric mandibular opening, and more symmetric facial structure/profile (Figures 1 and 2). None of the patients showed signs of relapse at one year following correction.Figure 1: Figure 1.Figure 2: Figure 2.CONCLUSIONS: Teenagers with severe hemifacial microsomia present reconstructive challenges, possessing a composite soft tissue and bone defect. The bony deficiencies underlying a tight soft tissue envelope can be prone to relapse. Stage I of our protocol addresses the occlusal cant and jaw malposition with orthognathic surgery. The RED device with anchorage in the mandible stabilizes the orthognathic jaw position, and prevents relapse that may occur without a mandibular ramus. Stage II establishes mandibular continuity, and adds soft tissue with a free fibular flap, the RED device is removed without fear of relapse. Later stages allow soft tissue contouring, creation of normal chin position, and autologous fat transfer. Using this sequential multi-staged approach for the patient with a complex composite defect from hemifacial microsomia, allows for a stable and normalized skeletal anatomy with adequate overlying soft tissue.'"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c63bfef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs = []\n",
    "for i in df['abstract']:\n",
    "    abs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2d2d0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BACKGROUND and PURPOSE: Teenagers with severe hemifacial microsomia can present with complex deformities of skin, adipose layer, mandible, and occlusion. Various individual techniques have been described to treat specific aspects of the deformity, including orthognathic surgery, distraction, bone grafting (vascularized/nonvascularized), and autologous fat transfer.1-3 Total correction of composite deficiencies, including creation of a proper skeletal foundation and soft tissue envelope, without relapse, is challenging in a single stage. We present our approach to teenagers with severe hemifacial microsomia incorporating orthognathic surgery, a rigid external distractor (RED) device, free osteoseptocutaneous fibular transfer, and fat grafting, to achieve stable skeletal and soft tissue correction.\n",
      "      METHODS: 3 teenage patients with severe hemifacial microsomia (Pruzansky 3) were treated at Dell Children\u0019s Medical Center with a sequential multi-staged approach for both skeletal and s\n"
     ]
    }
   ],
   "source": [
    "# Collect all abstracts into a single text blob\n",
    "abs = df['abstract'].tolist()\n",
    "all_text = \" \".join(abs)\n",
    "\n",
    "# `all_text` now contains the full text of all abstracts\n",
    "print(all_text[:1000])  # Show a preview of the combined text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "a146d0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_text_path = \"E:\\\\dataScience\\\\Fiver orders\\\\Order 3 Betopic modeling Plastic surgery\\\\all_abs.txt\"\n",
    "with open(abs_text_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(all_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df7bdb",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#121212; border-left: 5px solid #00bfbf; padding: 1.5em; font-family: 'Segoe UI', sans-serif; color: #e0e0e0; line-height:1.7;\">\n",
    "\n",
    "<h2 style=\"color:#00e6e6;\">üßπ Preprocessing Pipeline Before Topic Modeling</h2>\n",
    "\n",
    "<p>This project involves advanced cleaning and structuring of plastic surgery abstracts to ensure high-quality input for topic modeling using BERTopic. Each step below is critical to eliminate noise and enhance semantic clarity.</p>\n",
    "\n",
    "<hr style=\"border:none; border-top: 1px dashed #666;\">\n",
    "\n",
    "<h3 style=\"color:#ffffff;\">‚öôÔ∏è Steps Applied:</h3>\n",
    "<ol>\n",
    "  <li><strong>üóÉ Drop Empty Columns:</strong> Remove unused or null-filled columns (e.g., \"Unnamed: 2\").</li>\n",
    "  <li><strong>üîª Lowercase Text:</strong> Normalize casing to reduce duplication (e.g., ‚ÄúSurgery‚Äù vs ‚Äúsurgery‚Äù).</li>\n",
    "  <li><strong>üßΩ Remove Boilerplate Headers:</strong> Strip section headers such as <code>BACKGROUND:</code>, <code>METHODS:</code>, <code>CONCLUSION:</code>, etc.</li>\n",
    "  <li><strong>‚úÇÔ∏è Remove Punctuation & Digits:</strong> Eliminate characters irrelevant to topic formation.</li>\n",
    "  <li><strong>üß† Lemmatization:</strong> Convert words to their base form using <code>spaCy</code> (e.g., \"treated\" ‚Üí \"treat\").</li>\n",
    "  <li><strong>üö´ Stopword Removal:</strong> Use both <code>ENGLISH_STOP_WORDS</code> and a domain-specific list to remove high-frequency medical filler terms (e.g., \"patient\", \"procedure\", \"outcome\").</li>\n",
    "  <li><strong>üìè Filter Short Texts:</strong> Discard abstracts with fewer than 30 meaningful tokens to avoid noise in topic modeling.</li>\n",
    "  <li><strong>üß¨ Remove Duplicates:</strong> Drop identical or highly similar abstracts to avoid over-weighting.</li>\n",
    "</ol>\n",
    "\n",
    "<hr style=\"border:none; border-top: 1px dashed #666;\">\n",
    "\n",
    "<p style=\"font-size:0.95em;color:#aaa;\"><em>Note:</em> These steps ensure that BERTopic clusters are formed on meaningful linguistic signals rather than formatting or redundant content.</p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "bec84e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[col for col in df.columns if 'Unnamed' in col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4c77f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_boilerplate(text):\n",
    "    return re.sub(\n",
    "        r\"\\b(background|methods|results|conclusions|purpose|discussion|study design|figure|table|summary|introduction|objectives|design|references|study population|statistical analysis|data availability|acknowledgement|clinical question|study results|study objective)\\b[\\s:‚Äì\\-]*\",\n",
    "        \" \",\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7a6ec45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = clean_boilerplate(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "df['cleaned_abs']= df['abstract'].apply(clean_boilerplate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "05aba2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "custom_stopwords = ENGLISH_STOP_WORDS.union([\n",
    "    'purpose', 'methods', 'method', 'results', 'conclusion', 'patient', 'patients',\n",
    "    'study', 'studies', 'clinical', 'analysis', 'significant', 'outcomes',\n",
    "    'surgery', 'surgeon', 'surgeons', 'treatment', 'procedures', 'case', 'cases',\n",
    "    'including', 'performed', 'approach', 'report', 'data', 'number', 'using',\n",
    "    'compared', 'included', 'surgical', 'underwent', 'group', 'significantly',\n",
    "    'md', 'authors', 'references', 'and', 'of', 'to', 'the', 'in', 'for', 'on',\n",
    "    'with', 'as', 'by', 'at', 'from', 'a', 'an', 'is', 'was', 'are', 'be', 'this',\n",
    "    'that', 'it', 'we', 'they', 'their', 'or'\n",
    "])\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in custom_stopwords and len(t) > 2]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"cleaned_abs\"] = df[\"cleaned_abs\"].apply(tokenize_and_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "89c6f498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Teenagers severe hemifacial microsomia present...\n",
       "1    160,000 hip knee replacements year UK. After m...\n",
       "2    3,800 diagnosed sarcoma year 10% requiring spe...\n",
       "3    350,000 ventral hernia repairs (VHR) yearly US...\n",
       "4    challenge education offering adequate hands-on...\n",
       "Name: cleaned_abs, dtype: object"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_abs'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "a04f2350",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'e:\\\\dataScience\\\\Fiver orders\\\\Order 3 Betopic modeling Plastic surgery\\\\Plastic_Surgery_topicModeling\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[244]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     lemmatized = [lemmatizer.lemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(lemmatized)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlemmatized_abs\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcleaned_abs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatize_text_simple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mlemmatized_abs\u001b[39m\u001b[33m'\u001b[39m].head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[244]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mlemmatize_text_simple\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlemmatize_text_simple\u001b[39m(text):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     lemmatized = [lemmatizer.lemmatize(token) \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m.join(lemmatized)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'e:\\\\dataScience\\\\Fiver orders\\\\Order 3 Betopic modeling Plastic surgery\\\\Plastic_Surgery_topicModeling\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text_simple(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "df['lemmatized_abs'] = df['cleaned_abs'].apply(lemmatize_text_simple)\n",
    "df['lemmatized_abs'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40db757f",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "# There is no error shown in the provided context.\n",
    "# If you encountered an error, please provide the error message for specific help.\n",
    "# If you are referring to a variable or file not found, ensure all previous cells are executed in order.\n",
    "# If you meant to check for a specific error, please clarify.---------------------------------------------------------------------------\n",
    "LookupError                               Traceback (most recent call last)\n",
    "Cell In[244], line 8\n",
    "      5     lemmatized = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "      6     return \" \".join(lemmatized)\n",
    "----> 8 df['lemmatized_abs'] = df['cleaned_abs'].apply(lemmatize_text_simple)\n",
    "      9 df['lemmatized_abs'].head()\n",
    "\n",
    "File e:\\dataScience\\Fiver orders\\Order 3 Betopic modeling Plastic surgery\\Plastic_Surgery_topicModeling\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:4935, in Series.apply(self, func, convert_dtype, args, by_row, **kwargs)\n",
    "   4800 def apply(\n",
    "   4801     self,\n",
    "   4802     func: AggFuncType,\n",
    "   (...)   4807     **kwargs,\n",
    "   4808 ) -> DataFrame | Series:\n",
    "   4809     \"\"\"\n",
    "   4810     Invoke function on values of Series.\n",
    "   4811 \n",
    "   (...)   4926     dtype: float64\n",
    "   4927     \"\"\"\n",
    "   4928     return SeriesApply(\n",
    "   4929         self,\n",
    "   4930         func,\n",
    "   4931         convert_dtype=convert_dtype,\n",
    "   4932         by_row=by_row,\n",
    "   4933         args=args,\n",
    "...\n",
    "\n",
    "  Searched in:\n",
    "    - 'e:\\\\dataScience\\\\Fiver orders\\\\Order 3 Betopic modeling Plastic surgery\\\\Plastic_Surgery_topicModeling\\\\nltk_data'\n",
    "**********************************************************************\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde2acf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bertopic_env)",
   "language": "python",
   "name": ".vemv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
